---
order: 2
format:
    html:
        toc: false
original: https://christophm.github.io/interpretable-ml-book/intro.html
---

<!--{{< include ../_in_progress.qmd >}}-->
{{< include ../_original.qmd >}}

# 2 - Introduction

Ce livre vous explique comment rendre les modèles d'apprentissage automatique (supervisé) interprétables. Les chapitres contiennent des formules mathématiques, mais vous devriez être capable de comprendre les idées derrière les méthodes même sans ces formules. Ce livre n'est pas destiné aux personnes qui tentent d'apprendre l'apprentissage automatique à partir de zéro. Si vous êtes novice sur ce sujet, il existe de nombreux ouvrages et autres ressources pour apprendre les bases.
Je recommande le livre [*"The Elements of Statistical Learning"* par Hastie, Tibshirani, and Friedman (2009)](https://hastie.su.domains/ElemStatLearn/)[^Hastie] \[NdT : en anglais] et le cours en ligne d'[Andrew Ng's "*Machine Learning" online course*](https://www.coursera.org/learn/machine-learning) sur la plateforme [coursera.com](https://www.coursera.com) \[NdT : en anglais] pour débuter en apprentissage machine.
Les deux sont disponibles gratuitement.

De nouvelles méthodes d'interprétation des modèles d'apprentissage automatique sont publiées à une vitesse vertigineuse. Suivre tout ce qui est publié serait de la folie et tout simplement impossible. C'est pourquoi vous ne trouverez pas dans ce livre les méthodes les plus récentes et les plus sophistiquées, mais des méthodes établies et des concepts de base de l'interprétabilité de l'apprentissage automatique. Ces bases vous prépareront à rendre les modèles interprétables. Intégrer ces concepts de base vous permettra également de mieux comprendre et évaluer tout nouvel article sur l'interprétabilité publié sur [arxiv.org](https://arxiv.org/) dans les 5 dernières minutes depuis que vous avez commencé à lire ce livre (j'exagère peut-être légèrement le taux de publication).

Ce livre commence avec quelques [courtes histoires](/02-introduction/02.1-short_stories.qmd) dystopiques qu'il n'est pas nécessaire de lire pour comprendre le livre, mais qui, je l'espère, vous divertiront, mais aussi qu'elles vous entraineront dans la réflexion. Ensuite, le livre explore les concepts de [l'interprétabilité de l'apprentissage automatique](/03-interpretability/index.qmd).
Nous y discutons des situations où l'interprétabilité est importante, et des différents types d'explications qui existent. Les termes utilisés tout au long du livre peuvent être consultés dans la [section consacrée à la terminologie](/02-introduction/02.3-terminology.qmd).
La plupart des modèles et des méthodes expliqués sont présentés en utilisant des exemples de données réelles qui sont décrites dans le [chapitre concenant les jeux de données](/04-datasets/index.qmd).
Une façon de rendre l'apprentissage automatique interprétable est d'utiliser des [modèles réputés interprétables](/05-interpretable_models/index.qmd), tels que les modèles linéaires ou les arbres de décision. Une autre option est l'utilisation d'[outils d'interprétation agnostiques au modèle](/06-model_agnostic_methods/index.qmd) qui peuvent être appliqués à tout modèle d'apprentissage automatique supervisé.
Les méthodes agnostiques peuvent être divisées en [méthodes globales](/08-global_model_agnostic_methods/index.qmd) qui décrivent le comportement moyen du modèle, et en [méthodes locales](/09-local_model_agnostic_methods/index.qmd) qui expliquent les prédictions individuellement.
Le chapitre sur les méthodes agnostiques au modèle traite de méthodes telles que les [graphiques de dépendance partielle](/08-global_model_agnostic_methods/08.1-pdp.qmd) et l'[importance des caractéristiques](/08-global_model_agnostic_methods/08.5-permutation-feature-importance.qmd). Les méthodes agnostiques fonctionnent en modifiant l'entrée du modèle d'apprentissage automatique afin de mesurer les changements de la prédiction en sortie.
Le livre se termine par une perspective optimiste sur ce à quoi pourrait ressembler [l'avenir de l'apprentissage automatique interprétable](/11-future/index.qmd).

Vous pouvez soit lire le livre du début à la fin, soit passer directement aux méthodes qui vous intéressent.

J'espère que vous apprécierez la lecture de ce livre!

<!-- REFERENCES -->

[^Hastie]: Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. "The elements of statistical learning". [hastie.su.domains/ElemStatLearn](https://hastie.su.domains/ElemStatLearn/). hastie.su.domains/ElemStatLearn (2009).
