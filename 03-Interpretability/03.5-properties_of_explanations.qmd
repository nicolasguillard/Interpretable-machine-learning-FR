---
lang: fr
format:
    html:
        toc: false
---

{{< include ../_in_progress.qmd >}}

## 3.5 - Propriétés des Explications

Nous voulons expliquer les prédictions d'un modèle d'apprentissage automatique. Pour cela, nous nous appuyons sur une méthode d'explication, qui est un algorithme générant des explications. **Une explication relie généralement les valeurs des caractéristiques d'une instance à sa prédiction par le modèle d'une manière compréhensible par l'humain.** D'autres types d'explications consistent en un ensemble d'instances de données (par exemple dans le cas du modèle des k-plus proches voisins). Par exemple, nous pourrions prédire le risque de cancer en utilisant une machine à vecteurs de support et expliquer les prédictions en utilisant la [méthode des modèles locaux substituts](/09-local_model_agnostic_methods/09.2-lime.qmd), qui génère des arbres de décision comme explications. Ou nous pourrions utiliser un modèle de régression linéaire au lieu d'une machine à vecteurs de support. Le modèle de régression linéaire est déjà équipé d'une méthode d'explication (interprétation des poids).

Nous examinons de plus près les propriétés des méthodes d'explication et des explications (Robnik-Sikonja et Bohanec, 2018[^human-ml]). Ces propriétés peuvent être utilisées pour juger de la qualité d'une méthode d'explication ou d'une explication. Il n'est pas clair pour toutes ces propriétés comment les mesurer correctement, l'un des défis étant de formaliser comment elles pourraient être calculées.

**Propriétés des Méthodes d'Explication**

- **Puissance expressive** est le "langage" ou la structure des explications que la méthode est capable de générer. Une méthode d'explication pourrait générer des règles SI-ALORS, des arbres de décision, une somme pondérée, un langage naturel ou autre chose.
- **Translucidité** décrit à quel point la méthode d'explication dépend de l'examen du modèle d'apprentissage automatique, comme ses paramètres. Par exemple, les méthodes d'explication reposant sur des modèles intrinsèquement interprétables comme le modèle de régression linéaire (spécifiques au modèle) sont très translucides. Les méthodes ne reposant que sur la manipulation des entrées et l'observation des prédictions ont une translucidité nulle. Selon le scénario, différents niveaux de translucidité peuvent être souhaitables. L'avantage d'une forte translucidité est que la méthode peut s'appuyer sur plus d'informations pour générer des explications. L'avantage d'une faible translucidité est que la méthode d'explication est plus portable.
- **Portabilité** décrit la gamme de modèles d'apprentissage automatique avec lesquels la méthode d'explication peut être utilisée. Les méthodes ayant une faible translucidité ont une plus grande portabilité car elles traitent le modèle d'apprentissage automatique comme une boîte noire. Les modèles substituts pourraient être la méthode d'explication avec la plus grande portabilité. Les méthodes qui ne fonctionnent que pour, par exemple, les réseaux neuronaux récurrents ont une faible portabilité.
- **Complexité algorithmique** décrit la complexité computationnelle de la méthode qui génère l'explication. Cette propriété est importante à considérer lorsque le temps de calcul est un goulot d'étranglement dans la génération d'explications.

**Propriétés des Explications Individuelles**

- **Précision** : À quel point une explication prédit-elle bien des données non vues ? Une haute précision est particulièrement importante si l'explication est utilisée pour des prédictions à la place du modèle d'apprentissage automatique. Une faible précision peut être acceptable si la précision du modèle d'apprentissage automatique est également faible, et si l'objectif est d'expliquer ce que fait le modèle boîte noire. Dans ce cas, seule la fidélité est importante.
- **Fidélité** : À quel point l'explication approxime-t-elle la prédiction du modèle boîte noire ? Une haute fidélité est l'une des propriétés les plus importantes d'une explication, car une explication avec une faible fidélité est inutile pour expliquer le modèle d'apprentissage automatique. Précision et fidélité sont étroitement liées. Si le modèle boîte noire a une haute précision et que l'explication a une haute fidélité, l'explication a également une haute précision. Certaines explications offrent seulement une fidélité locale, ce qui signifie que l'explication n'approxime bien la prédiction du modèle que pour un sous-ensemble des données (p. ex. les [modèles substituts locaux](/09-local_model_agnostic_methods/09.2-lime.qmd)) ou même pour une seule instance de données (par exemple, [Valeurs de Shapley](/09-local_model_agnostic_methods/09.5-shapley.qmd)).
- **Consistance** : À quel point une explication diffère-t-elle entre des modèles qui ont été entraînés sur la même tâche et qui produisent des prédictions similaires ? Par exemple, j'entraîne une machine à vecteurs de support et un modèle de régression linéaire sur la même tâche et les deux produisent des prédictions très similaires. Je calcule des explications en utilisant une méthode de mon choix et analyse à quel point les explications sont différentes. Si les explications sont très similaires, les explications sont très cohérentes. Je trouve cette propriété quelque peu délicate, car les deux modèles pourraient utiliser différentes caractéristiques, mais obtenir des prédictions similaires (également appelé ["Effet Rashomon"](https://en.wikipedia.org/wiki/Rashomon_effect)). Dans ce cas, une haute consistance n'est pas souhaitable car les explications doivent être très différentes. Une haute consistance est souhaitable si les modèles se basent vraiment sur des relations similaires.
- **Stabilité** : À quel point les explications pour des instances similaires sont-elles similaires ? Alors que la consistance compare les explications entre modèles, la stabilité compare les explications entre des instances similaires pour un modèle fixe. Une haute stabilité signifie que de légères variations dans les caractéristiques d'une instance ne changent pas substantiellement l'explication (à moins que ces légères variations ne changent également fortement la prédiction). Un manque de stabilité peut être le résultat d'une haute variance de la méthode d'explication. En d'autres termes, la méthode d'explication est fortement affectée par de légers changements des valeurs des caractéristiques de l'instance à expliquer. Un manque de stabilité peut également être causé par des composants non déterministes de la méthode d'explication, comme une étape d'échantillonnage des données, comme la [méthode des modèles substituts locaux](/09-local_model_agnostic_methods/09.2-lime.qmd) l'utilise. Une haute stabilité est toujours souhaitable.
- **Compréhensibilité** : Dans quelle mesure les humains comprennent-ils les explications ? Cela ressemble à une propriété parmi tant d'autres, mais c'est l'éléphant dans la pièce. Difficile à définir et à mesurer, mais extrêmement important à bien faire. Beaucoup de gens s'accordent à dire que la compréhensibilité dépend du public. Des idées pour mesurer la compréhensibilité incluent la mesure de la taille de l'explication (nombre de caractéristiques avec un poids non nul dans un modèle linéaire, nombre de règles de décision, ...) ou tester à quel point les gens peuvent prédire le comportement du modèle d'apprentissage automatique à partir des explications. La compréhensibilité des caractéristiques utilisées dans l'explication devrait également être considérée. Une transformation complexe des caractéristiques peut être moins compréhensible que les caractéristiques originales.
- **Certitude** : L'explication reflète-t-elle la certitude du modèle d'apprentissage automatique ? De nombreux modèles d'apprentissage automatique ne donnent que des prédictions sans déclaration sur la confiance du modèle que la prédiction est correcte. Si le modèle prédit une probabilité de $4\%$ de cancer pour un patient, est-il aussi certain que la probabilité de $4\%$ qu'un autre patient, avec des valeurs de caractéristiques différentes, a reçue ? Une explication qui inclut la certitude du modèle est très utile.
- **Degré d'Importance** : Dans quelle mesure l'explication reflète-t-elle l'importance des caractéristiques ou des parties de l'explication ? Par exemple, si une règle de décision est générée comme explication pour une prédiction individuelle, est-il clair quelle condition de la règle était la plus importante ?
- **Nouveauté** : L'explication reflète-t-elle si une instance de données à expliquer provient d'une région "nouvelle" éloignée de la distribution des données d'entraînement ? Dans de tels cas, le modèle peut être inexact et l'explication peut être inutile. Le concept de nouveauté est lié au concept de certitude. Plus la nouveauté est élevée, plus il est probable que le modèle aura une faible certitude en raison du manque de données.
- **Représentativité** : Combien d'instances une explication couvre-t-elle ? Les explications peuvent couvrir l'ensemble du modèle (par exemple, l'interprétation des poids dans un modèle de régression linéaire) ou ne représenter qu'une prédiction individuelle (par exemple, [Valeurs de Shapley](/09-local_model_agnostic_methods/09.5-shapley.qmd)).

<!-- REFERENCES -->

[^human-ml]: Robnik-Sikonja, Marko, and Marko Bohanec. "Perturbation-based explanations of prediction models." Human and Machine Learning. Springer, Cham. 159-175. (2018).
